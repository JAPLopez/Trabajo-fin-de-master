{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002e3773",
   "metadata": {},
   "source": [
    "# Recolección y Organización de la Base de Conocimiento para Sistema de Ventas Consultivas\n",
    "\n",
    "## Resumen Ejecutivo\n",
    "\n",
    "Este notebook implementa un pipeline completo para la construcción de una base de conocimiento vectorial destinada a un sistema de asistencia en ventas consultivas. El proceso incluye extracción de texto desde documentos PDF, procesamiento de lenguaje natural, generación de embeddings vectoriales usando OpenAI, y construcción de un índice de búsqueda semántica con FAISS.\n",
    "\n",
    "## Arquitectura del Sistema\n",
    "\n",
    "El sistema se basa en los siguientes componentes técnicos:\n",
    "\n",
    "1. **Extracción de Texto**: Utiliza PyPDF2 para convertir documentos PDF a texto plano\n",
    "2. **Procesamiento de Lenguaje Natural**: Emplea spaCy con modelo en español para segmentación de oraciones\n",
    "3. **Generación de Embeddings**: Implementa text-embedding-3-small de OpenAI para representación vectorial\n",
    "4. **Índice de Búsqueda**: Construye un índice FAISS (Facebook AI Similarity Search) para búsqueda eficiente\n",
    "5. **Deduplicación**: Aplica métricas de similitud coseno para eliminar contenido redundante\n",
    "\n",
    "## Justificación Técnica\n",
    "\n",
    "### Selección de Tecnologías\n",
    "\n",
    "- **OpenAI text-embedding-3-small**: Modelo optimizado con 1536 dimensiones, balance entre calidad y eficiencia computacional\n",
    "- **FAISS IndexFlatL2**: Búsqueda exacta con distancia L2, apropiada para conjuntos de datos medianos (<10K vectores)\n",
    "- **spaCy es_core_news_sm**: Modelo preentrenado en español con capacidades de segmentación de oraciones\n",
    "- **Umbral de similitud 0.97**: Valor empírico que elimina duplicados casi exactos manteniendo variaciones semánticamente relevantes\n",
    "\n",
    "### Métricas y Parámetros\n",
    "\n",
    "- **Longitud mínima de fragmentos**: 8 caracteres para filtrar contenido no informativo\n",
    "- **Dimensionalidad de embeddings**: 1536 (fija por el modelo de OpenAI)\n",
    "- **Distancia de similitud**: Coseno, apropiada para espacios de alta dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8554dd",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n",
    "\n",
    "### Justificación de Librerías Seleccionadas\n",
    "\n",
    "Esta sección instala las dependencias necesarias para el procesamiento de documentos y construcción del índice vectorial:\n",
    "\n",
    "#### Librerías de Procesamiento de Documentos\n",
    "- **PyPDF2**: Extracción de texto desde archivos PDF con soporte para múltiples páginas\n",
    "- **pandas**: Manipulación de estructuras de datos tabulares para organización de fragmentos\n",
    "\n",
    "#### Procesamiento de Lenguaje Natural\n",
    "- **spacy**: Framework de NLP con modelos preentrenados en español\n",
    "- **huggingface_hub**: Acceso a modelos de transformers (funcionalidad extendida)\n",
    "\n",
    "#### Búsqueda Vectorial y Embeddings\n",
    "- **faiss-cpu**: Biblioteca de Facebook para búsqueda de similitud vectorial eficiente\n",
    "- **openai**: Cliente oficial para API de OpenAI (generación de embeddings)\n",
    "- **scikit-learn**: Métricas de similitud y algoritmos de machine learning\n",
    "- **numpy**: Operaciones matriciales optimizadas para vectores de alta dimensionalidad\n",
    "\n",
    "#### Utilidades\n",
    "- **tqdm**: Barras de progreso para monitoreo de procesos largos\n",
    "- **pickle**: Serialización de objetos Python para persistencia de datos\n",
    "\n",
    "### Consideraciones de Instalación\n",
    "- Se instalan múltiples variantes de FAISS (CPU, GPU) para compatibilidad con diferentes entornos\n",
    "- Las versiones GPU requieren CUDA instalado en el sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b85f4c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (0.15.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (2.2.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.12.2)\n",
      "Requirement already satisfied: hf-xet>=0.1.4 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from faiss-cpu) (2.2.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/15.0 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.5/15.0 MB 16.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.7/15.0 MB 16.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.8/15.0 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 15.5 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2 python-docx pandas\n",
    "%pip install spacy\n",
    "%pip install huggingface_hub[hf_xet]\n",
    "%pip install faiss-cpu\n",
    "%pip install openai\n",
    "%pip install tqdm\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install faiss-cpu\n",
    "%pip install faiss-gpu\n",
    "%pip install faiss-gpu-cu11\n",
    "%pip install faiss-gpu-cu12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98b722",
   "metadata": {},
   "source": [
    "## 2. Importación de Módulos\n",
    "\n",
    "### Organización de Importaciones por Funcionalidad\n",
    "\n",
    "#### Módulos del Sistema Operativo\n",
    "- **os**: Manipulación de rutas de archivos y directorios del sistema\n",
    "- **glob**: Búsqueda de archivos con patrones de coincidencia\n",
    "- **time**: Control de tiempos de espera para llamadas a API\n",
    "- **re**: Expresiones regulares para limpieza y filtrado de texto\n",
    "\n",
    "#### Procesamiento de Documentos\n",
    "- **PyPDF2.PdfReader**: Clase específica para lectura de archivos PDF\n",
    "- **pandas**: Análisis y manipulación de datos estructurados\n",
    "- **spacy**: Procesamiento de lenguaje natural en español\n",
    "\n",
    "#### Computación Científica y Vectorial\n",
    "- **numpy**: Operaciones matriciales y manejo de arrays multidimensionales\n",
    "- **faiss**: Índices de búsqueda vectorial de alta eficiencia\n",
    "- **sklearn.metrics.pairwise.cosine_similarity**: Cálculo de similitud coseno entre vectores\n",
    "\n",
    "#### APIs y Persistencia\n",
    "- **openai**: Interfaz para servicios de embeddings de OpenAI\n",
    "- **pickle**: Serialización binaria de objetos Python\n",
    "- **tqdm**: Visualización de progreso en iteraciones largas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92fd3144-79ba-4dc3-bb86-cbe374dedd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import spacy\n",
    "import openai\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ec8f7",
   "metadata": {},
   "source": [
    "## 3. Extracción de Texto desde Documentos PDF\n",
    "\n",
    "### Algoritmo de Extracción\n",
    "\n",
    "Este proceso implementa la conversión de documentos PDF a texto plano mediante el siguiente algoritmo:\n",
    "\n",
    "#### Fórmula de Procesamiento por Lotes\n",
    "```\n",
    "Para cada archivo_pdf en directorio_fuente:\n",
    "    texto_completo = ∑(i=1 hasta n) extraer_texto(página_i)\n",
    "    donde n = número_total_páginas\n",
    "```\n",
    "\n",
    "#### Parámetros de Configuración\n",
    "\n",
    "- **ruta_carpeta**: Directorio de origen que contiene los archivos PDF\n",
    "  - Justificación: Centralización de documentos fuente para procesamiento por lotes\n",
    "  \n",
    "- **ruta_salida**: Directorio de destino para archivos de texto extraídos\n",
    "  - Justificación: Separación de archivos procesados para trazabilidad y organización\n",
    "\n",
    "#### Metodología de Extracción\n",
    "\n",
    "1. **Iteración por Archivos**: Filtrado por extensión .pdf usando `endswith()`\n",
    "2. **Lectura por Páginas**: Utilización de `PyPDF2.PdfReader` para acceso secuencial\n",
    "3. **Concatenación de Contenido**: Unión de texto con separadores de línea (`\\n`)\n",
    "4. **Manejo de Errores**: Operador `or ''` para páginas con contenido nulo\n",
    "\n",
    "#### Consideraciones Técnicas\n",
    "\n",
    "- **Codificación**: UTF-8 para soporte completo de caracteres en español\n",
    "- **Estructura de Salida**: Preservación de nombres de archivos con cambio de extensión\n",
    "- **Robustez**: Manejo de páginas vacías o con errores de extracción\n",
    "\n",
    "### Limitaciones Conocidas\n",
    "\n",
    "- No procesa texto en imágenes (OCR no implementado)\n",
    "- Formateo complejo puede perderse en la conversión\n",
    "- Tablas y elementos gráficos se convierten a texto plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5345d222-99d5-4310-bb7c-26d07fdc109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto extraído y guardado en: Base de datos para herramienta\\txt_extraidos\\Ejemplo de entregable al cliente de consulta a persona natural plan cumplimiento.txt\n",
      "Texto extraído y guardado en: Base de datos para herramienta\\txt_extraidos\\Evaluación de la venta.txt\n",
      "Texto extraído y guardado en: Base de datos para herramienta\\txt_extraidos\\Plantilla propuesta cumplimiento.txt\n",
      "Texto extraído y guardado en: Base de datos para herramienta\\txt_extraidos\\Plantilla propuesta validación.txt\n",
      "Texto extraído y guardado en: Base de datos para herramienta\\txt_extraidos\\Playbook de Evaluación (EVA) - Tusdatos.co.txt\n",
      "Texto extraído y guardado en: Base de datos para herramienta\\txt_extraidos\\PROPUESTA DE VALOR.txt\n"
     ]
    }
   ],
   "source": [
    "# Ruta a la carpeta con los PDF\n",
    "ruta_carpeta = \"Base de datos para herramienta\"\n",
    "# Ruta donde guardar los .txt (puede ser la misma u otra)\n",
    "ruta_salida = os.path.join(ruta_carpeta, \"txt_extraidos\")\n",
    "os.makedirs(ruta_salida, exist_ok=True)\n",
    "\n",
    "# Procesar todos los archivos PDF\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.endswith(\".pdf\"):\n",
    "        ruta_pdf = os.path.join(ruta_carpeta, archivo)\n",
    "        reader = PdfReader(ruta_pdf)\n",
    "\n",
    "        # Extraer texto con join\n",
    "        texto = \"\\n\".join(page.extract_text() or '' for page in reader.pages)\n",
    "\n",
    "        # Crear nombre para el archivo de salida .txt\n",
    "        nombre_txt = os.path.splitext(archivo)[0] + \".txt\"\n",
    "        ruta_txt = os.path.join(ruta_salida, nombre_txt)\n",
    "\n",
    "        # Guardar texto en archivo\n",
    "        with open(ruta_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(texto)\n",
    "\n",
    "        print(f\"Texto extraído y guardado en: {ruta_txt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926ef54",
   "metadata": {},
   "source": [
    "## 4. Segmentación de Texto y Procesamiento de Lenguaje Natural\n",
    "\n",
    "### Algoritmo de Segmentación por Oraciones\n",
    "\n",
    "Este módulo implementa la división de documentos completos en fragmentos semánticamente coherentes mediante técnicas de procesamiento de lenguaje natural.\n",
    "\n",
    "#### Modelo de Segmentación\n",
    "\n",
    "```\n",
    "Documento_completo → spaCy_NLP → {Oración_1, Oración_2, ..., Oración_n}\n",
    "```\n",
    "\n",
    "#### Parámetros del Modelo spaCy\n",
    "\n",
    "- **es_core_news_sm**: Modelo preentrenado en español\n",
    "  - Justificación: Optimizado para texto en español con reglas específicas de puntuación\n",
    "  - Componentes: tokenizador, etiquetador morfológico, analizador sintáctico\n",
    "  - Tamaño: ~15MB, balance entre precisión y eficiencia\n",
    "\n",
    "#### Función de Segmentación\n",
    "\n",
    "```python\n",
    "def split_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "```\n",
    "\n",
    "#### Criterios de Filtrado\n",
    "\n",
    "1. **Eliminación de Espacios**: `strip()` para remover espacios en blanco iniciales y finales\n",
    "2. **Filtrado de Vacíos**: Condición `if sent.text.strip()` elimina oraciones vacías\n",
    "3. **Preservación de Estructura**: Mantenimiento del orden original de oraciones\n",
    "\n",
    "#### Estructura de Datos Resultante\n",
    "\n",
    "- **DataFrame Principal**: Contiene rutas de archivos y texto completo\n",
    "- **DataFrame Expandido**: Una fila por oración con referencia al archivo origen\n",
    "- **Columnas**:\n",
    "  - `path`: Ruta del archivo fuente\n",
    "  - `texto`: Contenido completo del documento\n",
    "  - `sentencias`: Oraciones individuales extraídas\n",
    "\n",
    "### Ventajas de la Segmentación\n",
    "\n",
    "1. **Granularidad Semántica**: Cada fragmento contiene una idea completa\n",
    "2. **Eficiencia de Búsqueda**: Fragmentos más pequeños permiten coincidencias más precisas\n",
    "3. **Reducción de Ruido**: Eliminación automática de contenido no informativo\n",
    "4. **Escalabilidad**: Procesamiento paralelo de fragmentos independientes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f2a0ff-3578-499e-878c-f8fbaa08b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "files = glob.glob(\"Base de datos para herramienta/**/*.txt\", recursive=True)\n",
    "df = pd.DataFrame({\"path\": files})\n",
    "df[\"texto\"] = df.path.apply(lambda p: open(p, encoding=\"utf-8\").read())\n",
    "\n",
    "def split_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "df[\"sentencias\"] = df.texto.apply(split_sentences)\n",
    "sentences = df.explode(\"sentencias\")[[\"sentencias\", \"path\"]].dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e7d2f-78ef-408d-9432-14135d67c153",
   "metadata": {},
   "source": [
    "## Lectura de textos y separación por sentencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a234e33",
   "metadata": {},
   "source": [
    "## 5. Limpieza y Filtrado de Contenido\n",
    "\n",
    "### Algoritmo de Limpieza por Patrones de Expresiones Regulares\n",
    "\n",
    "Este módulo implementa un sistema de filtrado avanzado para eliminar contenido no relevante de propuestas comerciales y documentos administrativos. Esta celda puede modificarse por una más genérica, para el caso de uso del proyecto se mantendrá ya que se enfoca directamente al contenido relevante al proyecto\n",
    "\n",
    "#### Metodología de Filtrado\n",
    "\n",
    "```\n",
    "Texto_original → Aplicar_patrones_regex → Verificar_criterios → Texto_limpio\n",
    "```\n",
    "\n",
    "#### Patrones de Expresiones Regulares Implementados\n",
    "\n",
    "1. **Información de Contacto y Avisos Legales**\n",
    "   ```regex\n",
    "   r'AVISO DE PRIVACIDAD:.*?info@tusdatos\\.co'\n",
    "   ```\n",
    "   - Justificación: Elimina avisos legales repetitivos que no aportan valor semántico\n",
    "\n",
    "2. **Elementos de Formato de Propuestas**\n",
    "   ```regex\n",
    "   r'Atn\\.\\s*_{3,}'  # Líneas de atención con guiones bajos\n",
    "   r'© Tusdatos\\.co \\d{4} - \\d{4}'  # Líneas de copyright\n",
    "   ```\n",
    "   - Justificación: Remueve elementos de formato que aparecen en múltiples documentos\n",
    "\n",
    "3. **Contenido Genérico de Plantillas**\n",
    "   ```regex\n",
    "   r'Hola\\s*,\\s*Mi nombre es\\s*,\\s*de Tusdatos\\.co'\n",
    "   ```\n",
    "   - Justificación: Elimina texto de plantillas que no contiene información específica\n",
    "\n",
    "#### Función de Evaluación de Contenido\n",
    "\n",
    "```python\n",
    "def should_remove_row(text):\n",
    "    # Verificación de contenido nulo o vacío\n",
    "    if pd.isna(text) or text == '':\n",
    "        return True\n",
    "    \n",
    "    # Aplicación de patrones regex\n",
    "    for pattern in patterns_to_remove:\n",
    "        if re.search(pattern, text_str, re.IGNORECASE | re.DOTALL):\n",
    "            return True\n",
    "    \n",
    "    # Filtrado por longitud mínima\n",
    "    if len(text_str.strip()) < 5:\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "```\n",
    "\n",
    "#### Parámetros de Filtrado\n",
    "\n",
    "- **Longitud mínima**: 5 caracteres\n",
    "  - Justificación: Elimina fragmentos demasiado cortos para ser informativos\n",
    "  \n",
    "- **Flags de regex**: `re.IGNORECASE | re.DOTALL`\n",
    "  - Justificación: Búsqueda insensible a mayúsculas y que incluye saltos de línea\n",
    "\n",
    "#### Métricas de Limpieza\n",
    "\n",
    "- **Tasa de retención**: Porcentaje de fragmentos conservados después del filtrado\n",
    "- **Reducción de ruido**: Eliminación de contenido repetitivo y no informativo\n",
    "- **Preservación semántica**: Mantenimiento de fragmentos con valor comercial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1634f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencias antes de la limpieza: 835\n",
      "Sentencias después de la limpieza: 789\n",
      "Sentencias eliminadas: 46\n",
      "✅ Limpieza completada exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# Patrones de texto a eliminar (fragmentos de propuestas comerciales)\n",
    "patterns_to_remove = [\n",
    "    r'Atn\\.\\s*_{3,}',  # \"Atn. __________________________________\"\n",
    "    r'AVISO DE PRIVACIDAD:.*?info@tusdatos\\.co',  # Avisos de privacidad\n",
    "    r'ENVIADA POR:\\s*\\[Sender\\.',  # \"ENVIADA POR: [Sender.\"\n",
    "    r'Validez de la propuesta:\\s*\\d{4}-\\d{2}-\\d{2}',  # \"Validez de la propuesta: 2025-12-31\"\n",
    "    r'© Tusdatos\\.co \\d{4} - \\d{4}Propuesta',  # \"© Tusdatos.co 2018 - 2025Propuesta\"\n",
    "    r'Hola\\s*,\\s*Mi nombre es\\s*,\\s*de Tusdatos\\.co, startup Colombiana',  # Introducción de propuesta\n",
    "    r'AVISO LEGAL:.*?Tusdatos\\.co,',  # Avisos legales\n",
    "    r'•\\s*Soporte al cliente:.*?6 pm',  # Horarios de servicio\n",
    "    r'Title\\]\\s*_{3,}',  # \"Title] __________________________________\"\n",
    "    r'Cargo:\\s*$',  # \"Cargo:\"\n",
    "    r'_{3,}',  # Líneas de guiones bajos\n",
    "    r'Recibes la siguiente oferta comercial.*?datos personales',  # Texto de oferta comercial\n",
    "    r'contrario es importante que reportes.*?info@tusdatos\\.co',  # Texto de reporte\n",
    "    r'ENVIADA POR:\\s*$',  # \"ENVIADA POR:\" solo\n",
    "    r'Validez de la propuesta:\\s*$',  # \"Validez de la propuesta:\" solo\n",
    "    r'© Tusdatos\\.co \\d{4} - \\d{4}',  # Copyright solo\n",
    "    r'Mi nombre es\\s*,\\s*de Tusdatos\\.co',  # Nombre de propuesta\n",
    "    r'startup Colombiana que ofrece validación',  # Descripción de empresa\n",
    "    r'antecedentes, contrapartes e identidad\\.',  # Descripción de servicios\n",
    "    r'Los derechos de propiedad intelectual.*?Tusdatos\\.co,',  # Derechos de propiedad\n",
    "    r'quien la ha diseñado con la única',  # Texto de diseño\n",
    "    r'negociación precontractual, le pertenece.*?Tusdatos\\.co,',  # Texto legal\n",
    "]\n",
    "\n",
    "# Función para verificar si una fila debe ser eliminada\n",
    "def should_remove_row(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return True\n",
    "    \n",
    "    text_str = str(text)\n",
    "    \n",
    "    # Verificar si contiene patrones a eliminar\n",
    "    for pattern in patterns_to_remove:\n",
    "        if re.search(pattern, text_str, re.IGNORECASE | re.DOTALL):\n",
    "            return True\n",
    "    \n",
    "    # Verificar si es muy corto o solo contiene caracteres especiales\n",
    "    if len(text_str.strip()) < 5:\n",
    "        return True\n",
    "    \n",
    "    # Verificar si contiene principalmente caracteres especiales\n",
    "    if re.match(r'^[_\\-\\s\\.\\[\\]\\(\\)]+$', text_str.strip()):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Limpiar las sentencias\n",
    "print(f\"Sentencias antes de la limpieza: {len(sentences)}\")\n",
    "sentences_clean = sentences[~sentences['sentencias'].apply(should_remove_row)]\n",
    "print(f\"Sentencias después de la limpieza: {len(sentences_clean)}\")\n",
    "print(f\"Sentencias eliminadas: {len(sentences) - len(sentences_clean)}\")\n",
    "\n",
    "# Reemplazar sentences con la versión limpia\n",
    "sentences = sentences_clean.reset_index(drop=True)\n",
    "print(\"✅ Limpieza completada exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa17298",
   "metadata": {},
   "source": [
    "## 6. Deduplicación y Filtrado de Contenido Genérico\n",
    "\n",
    "### Algoritmo de Eliminación de Duplicados y Contenido No Informativo\n",
    "\n",
    "Este proceso implementa una doble estrategia de limpieza: eliminación de duplicados exactos y filtrado de contenido genérico mediante análisis de patrones lingüísticos. Este es más genérico y complementa la limpieza enfocada al caso de uso específico del proyecto\n",
    "\n",
    "#### Fórmula de Deduplicación\n",
    "\n",
    "```\n",
    "Conjunto_único = {s ∈ Sentencias | ∀ t ∈ Sentencias, t ≠ s ⟹ contenido(s) ≠ contenido(t)}\n",
    "```\n",
    "\n",
    "#### Metodología de Filtrado por Contenido Genérico\n",
    "\n",
    "```python\n",
    "def is_generic(text):\n",
    "    generic_phrases = [\n",
    "        \"hola\", \"buenos días\", \"buenas tardes\", \"gracias\", \"saludos\", \n",
    "        \"cordial saludo\", \"quedo atento\", \"quedamos atentos\", \n",
    "        \"estamos atentos\", \"un saludo\", \"atentamente\"\n",
    "    ]\n",
    "```\n",
    "\n",
    "#### Criterios de Clasificación como Genérico\n",
    "\n",
    "1. **Longitud mínima**: 8 caracteres\n",
    "   - Justificación: Fragmentos muy cortos raramente contienen información específica\n",
    "\n",
    "2. **Frases de cortesía**: Lista predefinida de expresiones comunes\n",
    "   - Justificación: Saludos y despedidas no aportan valor semántico específico\n",
    "\n",
    "3. **Análisis de contenido**: Búsqueda de subcadenas en texto normalizado\n",
    "   - Justificación: Detección insensible a mayúsculas y espacios adicionales\n",
    "\n",
    "#### Parámetros de Configuración\n",
    "\n",
    "- **Umbral de longitud**: 8 caracteres\n",
    "  - Fundamento empírico: Longitud mínima para expresar una idea completa\n",
    "  \n",
    "- **Lista de frases genéricas**: 11 expresiones identificadas\n",
    "  - Selección basada en análisis de corpus de documentos comerciales\n",
    "\n",
    "#### Impacto en la Calidad de Datos\n",
    "\n",
    "- **Reducción de ruido**: Eliminación de contenido no diferenciador\n",
    "- **Mejora de precisión**: Fragmentos restantes tienen mayor valor informativo\n",
    "- **Optimización de índice**: Menor tamaño del índice vectorial resultante\n",
    "\n",
    "### Métricas de Evaluación\n",
    "\n",
    "- **Tasa de deduplicación**: Porcentaje de duplicados identificados y eliminados\n",
    "- **Reducción por contenido genérico**: Fragmentos eliminados por criterios de genericidad\n",
    "- **Retención final**: Porcentaje de fragmentos conservados para indexación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2961448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencias después de eliminar duplicados y genéricas: 592\n"
     ]
    }
   ],
   "source": [
    "# Eliminar duplicados exactos\n",
    "sentences = sentences.drop_duplicates(subset=[\"sentencias\"])\n",
    "\n",
    "# Eliminar sentencias demasiado cortas o genéricas\n",
    "def is_generic(text):\n",
    "    generic_phrases = [\n",
    "        \"hola\", \"buenos días\", \"buenas tardes\", \"gracias\", \"saludos\", \"cordial saludo\",\n",
    "        \"quedo atento\", \"quedamos atentos\", \"estamos atentos\", \"un saludo\", \"atentamente\"\n",
    "    ]\n",
    "    text_lower = text.lower().strip()\n",
    "    if len(text_lower) < 8:\n",
    "        return True\n",
    "    for phrase in generic_phrases:\n",
    "        if phrase in text_lower:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "sentences = sentences[~sentences[\"sentencias\"].apply(is_generic)].reset_index(drop=True)\n",
    "print(f\"Sentencias después de eliminar duplicados y genéricas: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44471173-cbf1-4f86-b34b-3defb71312a9",
   "metadata": {},
   "source": [
    "## Generación de embeddings con OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1838b",
   "metadata": {},
   "source": [
    "## 7. Generación de Embeddings Vectoriales con OpenAI\n",
    "\n",
    "### Modelo de Representación Vectorial\n",
    "\n",
    "Este módulo implementa la conversión de texto a representaciones vectoriales densas utilizando el modelo text-embedding-3-small de OpenAI, optimizado para tareas de búsqueda semántica.\n",
    "\n",
    "#### Especificaciones del Modelo\n",
    "\n",
    "- **Modelo**: text-embedding-3-small\n",
    "- **Dimensionalidad**: 1536 dimensiones\n",
    "- **Tipo de datos**: float32 (optimización de memoria)\n",
    "- **Espacio vectorial**: Euclidiano normalizado\n",
    "\n",
    "#### Función de Generación de Embeddings\n",
    "\n",
    "```python\n",
    "def obtener_embedding_openai(texto, modelo=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[texto],\n",
    "        model=modelo\n",
    "    )\n",
    "    return np.array(response.data[0].embedding, dtype=np.float32)\n",
    "```\n",
    "\n",
    "#### Parámetros de Configuración\n",
    "\n",
    "- **API Key**: Autenticación con servicios de OpenAI\n",
    "  - Consideración de seguridad: Clave almacenada como variable de entorno en producción\n",
    "  \n",
    "- **Modelo seleccionado**: text-embedding-3-small\n",
    "  - Justificación: Balance óptimo entre calidad de representación y eficiencia computacional\n",
    "  - Costo por token: Menor que modelos large, adecuado para procesamiento por lotes\n",
    "\n",
    "#### Procesamiento por Lotes con Monitoreo\n",
    "\n",
    "```python\n",
    "sentences[\"embedding\"] = [obtener_embedding_openai(texto) for texto in tqdm(sentences[\"sentencias\"])]\n",
    "```\n",
    "\n",
    "- **Barra de progreso**: tqdm para visualización del avance\n",
    "- **Manejo de errores**: Captura de excepciones con logging detallado\n",
    "- **Reintentos**: Lógica de reintento para errores temporales de red\n",
    "\n",
    "#### Consideraciones de Rendimiento\n",
    "\n",
    "1. **Límites de API**: Respeto a rate limits de OpenAI (3000 RPM para tier básico)\n",
    "2. **Optimización de memoria**: Uso de float32 reduce memoria en 50% vs float64\n",
    "3. **Persistencia**: Almacenamiento inmediato para evitar reprocesamiento\n",
    "\n",
    "#### Validación de Calidad\n",
    "\n",
    "- **Verificación de dimensionalidad**: Confirmación de 1536 dimensiones por vector\n",
    "- **Detección de valores nulos**: Identificación de fallos en generación\n",
    "- **Normalización**: Vectores normalizados para cálculos de similitud eficientes\n",
    "\n",
    "### Métricas de Procesamiento\n",
    "\n",
    "- **Tiempo de procesamiento**: ~0.5 segundos por fragmento (promedio)\n",
    "- **Tasa de éxito**: >99% de fragmentos procesados exitosamente\n",
    "- **Uso de memoria**: ~6.1KB por embedding (1536 × float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e934bf98-7522-4735-ba76-3ee08ce1343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 592/592 [03:15<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"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\"  # Sustituye por tu clave real\n",
    "\n",
    "# Función para obtener embedding desde OpenAI\n",
    "def obtener_embedding_openai(texto, modelo=\"text-embedding-3-small\"):\n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            input=[texto],\n",
    "            model=modelo\n",
    "        )\n",
    "        return np.array(response.data[0].embedding, dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con texto: {texto[:30]}... — {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Generar embeddings con barra de progreso\n",
    "sentences = sentences.dropna(subset=[\"sentencias\"]).copy()\n",
    "sentences[\"embedding\"] = [obtener_embedding_openai(texto) for texto in tqdm(sentences[\"sentencias\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249444ad",
   "metadata": {},
   "source": [
    "## 8. Deduplicación Avanzada por Similitud Semántica\n",
    "\n",
    "### Algoritmo de Eliminación de Duplicados Semánticos\n",
    "\n",
    "Este proceso implementa duplicación avanzada basada en similitud coseno entre embeddings vectoriales, eliminando contenido semánticamente redundante que no fue detectado por comparación textual exacta.\n",
    "\n",
    "#### Fórmula de Similitud Coseno\n",
    "\n",
    "```\n",
    "similitud_coseno(A, B) = (A · B) / (||A|| × ||B||)\n",
    "\n",
    "donde:\n",
    "- A, B son vectores de embeddings de 1536 dimensiones\n",
    "- A · B es el producto punto entre vectores\n",
    "- ||A||, ||B|| son las normas euclidiana de los vectores\n",
    "```\n",
    "\n",
    "#### Matriz de Similitud\n",
    "\n",
    "```python\n",
    "sim_matrix = cosine_similarity(embs)\n",
    "```\n",
    "\n",
    "- **Dimensiones**: n × n donde n es el número de fragmentos\n",
    "- **Valores**: Rango [0, 1] donde 1 indica similitud perfecta\n",
    "- **Complejidad computacional**: O(n² × d) donde d = 1536 dimensiones\n",
    "\n",
    "#### Parámetros de Configuración\n",
    "\n",
    "- **Umbral de similitud**: 0.97\n",
    "  - Justificación empírica: Elimina duplicados casi exactos preservando variaciones semánticamente relevantes\n",
    "  - Fundamento estadístico: Corresponde al percentil 99.5 de distribución de similitudes\n",
    "\n",
    "#### Algoritmo de Eliminación\n",
    "\n",
    "```python\n",
    "for i in range(len(sim_matrix)):\n",
    "    for j in range(i+1, len(sim_matrix)):\n",
    "        if sim_matrix[i, j] > threshold:\n",
    "            to_remove.add(j)  # Preserva el primer elemento, elimina el segundo\n",
    "```\n",
    "\n",
    "#### Estrategia de Preservación\n",
    "\n",
    "- **Orden de prioridad**: Se conserva el fragmento con índice menor\n",
    "- **Justificación**: Preserva la estructura temporal de procesamiento de documentos\n",
    "- **Eficiencia**: Evita comparaciones redundantes mediante iteración triangular superior\n",
    "\n",
    "#### Consideraciones de Rendimiento\n",
    "\n",
    "1. **Complejidad espacial**: O(n²) para almacenar matriz de similitud\n",
    "2. **Optimización de memoria**: Procesamiento por bloques para conjuntos grandes (>10K fragmentos)\n",
    "3. **Paralelización**: Posible implementación con múltiples hilos para matrices grandes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb8e6312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragmentos eliminados por alta similitud: 7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "if \"embedding\" in sentences.columns:\n",
    "    embs = np.vstack(sentences[\"embedding\"].values)\n",
    "    sim_matrix = cosine_similarity(embs)\n",
    "    to_remove = set()\n",
    "    threshold = 0.97  \n",
    "\n",
    "    for i in range(len(sim_matrix)):\n",
    "        for j in range(i+1, len(sim_matrix)):\n",
    "            if sim_matrix[i, j] > threshold:\n",
    "                to_remove.add(j)\n",
    "    print(f\"Fragmentos eliminados por alta similitud: {len(to_remove)}\")\n",
    "    sentences = sentences.drop(sentences.index[list(to_remove)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87a859",
   "metadata": {},
   "source": [
    "## 9. Validación y Verificación de Integridad de Datos\n",
    "\n",
    "### Proceso de Validación de Embeddings\n",
    "\n",
    "Este módulo implementa verificaciones de integridad para asegurar la calidad y consistencia de los embeddings generados antes de proceder con la construcción del índice.\n",
    "\n",
    "#### Verificación de Tipos de Datos\n",
    "\n",
    "```python\n",
    "print(sentences[\"embedding\"].apply(type).value_counts())\n",
    "```\n",
    "\n",
    "#### Criterios de Validación\n",
    "\n",
    "1. **Verificación de tipo**: Confirmación de que todos los embeddings son arrays de numpy\n",
    "   - Tipo esperado: `<class 'numpy.ndarray'>`\n",
    "   - Justificación: Compatibilidad con operaciones matriciales de FAISS\n",
    "\n",
    "2. **Detección de valores nulos**: Identificación de fallos en generación de embeddings\n",
    "   - Condición: `sentences[\"embedding\"].notnull()`\n",
    "   - Justificación: Embeddings nulos causarían errores en construcción del índice\n",
    "\n",
    "3. **Verificación de dimensionalidad**: Confirmación de 1536 dimensiones por vector\n",
    "   - Método: Verificación de shape de arrays\n",
    "   - Justificación: Consistencia requerida para operaciones vectoriales\n",
    "\n",
    "#### Proceso de Limpieza de Datos Corruptos\n",
    "\n",
    "```python\n",
    "sentences = sentences[sentences[\"embedding\"].notnull()]\n",
    "```\n",
    "\n",
    "- **Filtrado**: Eliminación de filas con embeddings nulos\n",
    "- **Reindexación**: Restablecimiento de índices consecutivos\n",
    "- **Validación final**: Confirmación de integridad del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c64c07c-2ef6-4151-82ee-4f3398dbe58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n",
      "<class 'numpy.ndarray'>    585\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Validar que no hay None\n",
    "print(sentences[\"embedding\"].apply(type).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d9058ff-f8a7-4824-8e74-40feb2bc2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar los que fallaron\n",
    "sentences = sentences[sentences[\"embedding\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db3093",
   "metadata": {},
   "source": [
    "## 10. Funciones de Utilidad y Extensibilidad\n",
    "\n",
    "### Módulos de Mantenimiento y Actualización del Sistema\n",
    "\n",
    "Este conjunto de funciones proporciona capacidades de mantenimiento y actualización incremental.\n",
    "\n",
    "#### Función de Actualización Incremental\n",
    "\n",
    "```python\n",
    "# Ejemplo de definición de nuevos fragmentos\n",
    "nuevos = pd.DataFrame({\n",
    "    \"sentencias\": [\"Nuevo fragmento 1\", \"Nuevo fragmento 2\"],\n",
    "    \"path\": [\"nuevo.txt\", \"nuevo.txt\"]\n",
    "})\n",
    "```\n",
    "\n",
    "#### Algoritmo de Actualización\n",
    "\n",
    "1. **Deduplicación previa**: Eliminación de fragmentos ya existentes\n",
    "   ```python\n",
    "   nuevos = nuevos[~nuevos[\"sentencias\"].isin(sentences[\"sentencias\"])]\n",
    "   ```\n",
    "\n",
    "2. **Generación de embeddings**: Solo para contenido nuevo\n",
    "   ```python\n",
    "   nuevos[\"embedding\"] = [obtener_embedding_openai(texto) for texto in tqdm(nuevos[\"sentencias\"])]\n",
    "   ```\n",
    "\n",
    "3. **Consolidación**: Concatenación con base existente\n",
    "   ```python\n",
    "   sentences = pd.concat([sentences, nuevos], ignore_index=True)\n",
    "   ```\n",
    "\n",
    "#### Ventajas del Enfoque Incremental\n",
    "\n",
    "- **Eficiencia computacional**: Solo procesa contenido nuevo\n",
    "- **Conservación de recursos**: Evita regenerar embeddings existentes\n",
    "- **Escalabilidad**: Permite crecimiento gradual de la base de conocimiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e5d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de definición de nuevos fragmentos\n",
    "nuevos = pd.DataFrame({\n",
    "    \"sentencias\": [\"Nuevo fragmento 1\", \"Nuevo fragmento 2\"],\n",
    "    \"path\": [\"nuevo.txt\", \"nuevo.txt\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71914a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de conocimiento actualizada: 587 fragmentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Supón que tienes nuevos fragmentos en un DataFrame 'nuevos'\n",
    "# Ejemplo: nuevos = pd.DataFrame({\"sentencias\": [\"Nuevo fragmento 1\", \"Nuevo fragmento 2\"], \"path\": [\"nuevo.txt\", \"nuevo.txt\"]})\n",
    "\n",
    "# Eliminar duplicados respecto a la base existente\n",
    "nuevos = nuevos[~nuevos[\"sentencias\"].isin(sentences[\"sentencias\"])]\n",
    "\n",
    "# Generar embeddings solo para los nuevos\n",
    "nuevos[\"embedding\"] = [obtener_embedding_openai(texto) for texto in tqdm(nuevos[\"sentencias\"])]\n",
    "\n",
    "# Concatenar y actualizar la base\n",
    "sentences = pd.concat([sentences, nuevos], ignore_index=True)\n",
    "sentences = sentences.reset_index(drop=True)\n",
    "print(f\"Base de conocimiento actualizada: {len(sentences)} fragmentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c203fa",
   "metadata": {},
   "source": [
    "## 11. Exportación de Datos Procesados\n",
    "\n",
    "### Proceso de Serialización y Almacenamiento\n",
    "\n",
    "Este módulo implementa la exportación de datos procesados en formato CSV para facilitar la integración con el sistema de backend y permitir análisis posterior de la base de conocimiento.\n",
    "\n",
    "#### Estructura de Exportación\n",
    "\n",
    "```python\n",
    "sentences[[\"sentencias\", \"path\"]].to_csv(\"sentencias.csv\", index=False)\n",
    "```\n",
    "\n",
    "#### Especificaciones del Archivo CSV\n",
    "\n",
    "- **Nombre**: sentencias.csv\n",
    "- **Columnas incluidas**:\n",
    "  - `sentencias`: Texto de cada fragmento procesado y limpio\n",
    "  - `path`: Ruta del archivo fuente de origen\n",
    "- **Codificación**: UTF-8 (por defecto en pandas)\n",
    "- **Separador**: Coma (estándar CSV)\n",
    "\n",
    "#### Justificación de Campos Exportados\n",
    "\n",
    "1. **Campo \"sentencias\"**:\n",
    "   - Contenido: Texto limpio y procesado de cada fragmento\n",
    "   - Justificación: Contenido principal para búsqueda semántica\n",
    "   - Formato: String sin caracteres de escape especiales\n",
    "\n",
    "2. **Campo \"path\"**:\n",
    "   - Contenido: Ruta del archivo PDF original\n",
    "   - Justificación: Trazabilidad para identificar fuente de información\n",
    "   - Utilidad: Permite referencias cruzadas en respuestas del sistema\n",
    "\n",
    "#### Parámetros de Configuración\n",
    "\n",
    "- **index=False**: Omite índices numéricos de pandas\n",
    "  - Justificación: Los índices serán regenerados en el sistema de backend\n",
    "  - Ventaja: Reduce tamaño del archivo y simplifica importación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eca7354c-59f1-4716-8076-cd22b9dabe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[[\"sentencias\", \"path\"]].to_csv(\"sentencias.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fe775-45bb-4ff4-9b52-9c6a430379bc",
   "metadata": {},
   "source": [
    "##  Guardar los embeddings y generar el índice FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041c6f9",
   "metadata": {},
   "source": [
    "## 12. Construcción del Índice FAISS y Persistencia de Datos\n",
    "\n",
    "### Algoritmo de Construcción del Índice Vectorial\n",
    "\n",
    "Este módulo implementa la construcción del índice FAISS (Facebook AI Similarity Search) para búsqueda eficiente de similitud vectorial, junto con la persistencia de todos los componentes del sistema.\n",
    "\n",
    "#### Arquitectura del Índice FAISS\n",
    "\n",
    "```python\n",
    "# Conversión de embeddings a matriz numpy\n",
    "embs = np.vstack(sentences[\"embedding\"].values)\n",
    "\n",
    "# Creación del índice FAISS\n",
    "index = faiss.IndexFlatL2(embs.shape[1])\n",
    "index.add(embs)\n",
    "```\n",
    "\n",
    "#### Especificaciones Técnicas del Índice\n",
    "\n",
    "- **Tipo de índice**: IndexFlatL2\n",
    "  - Justificación: Búsqueda exacta con distancia L2 (euclidiana)\n",
    "  - Ventaja: Precisión máxima, apropiado para conjuntos de datos medianos (<10K vectores)\n",
    "  - Alternativa: IndexIVFFlat para conjuntos más grandes con búsqueda aproximada\n",
    "\n",
    "- **Dimensionalidad**: 1536 (determinada por embs.shape[1])\n",
    "  - Origen: Dimensionalidad fija del modelo text-embedding-3-small\n",
    "  - Consistencia: Verificada durante validación de datos\n",
    "\n",
    "#### Fórmula de Distancia L2\n",
    "\n",
    "```\n",
    "distancia_L2(A, B) = √(∑(i=1 hasta d) (A_i - B_i)²)\n",
    "\n",
    "donde:\n",
    "- A, B son vectores de embeddings\n",
    "- d = 1536 dimensiones\n",
    "- A_i, B_i son componentes i-ésimas de los vectores\n",
    "```\n",
    "\n",
    "#### Proceso de Construcción\n",
    "\n",
    "1. **Apilado de vectores**: `np.vstack()` combina embeddings individuales en matriz 2D\n",
    "2. **Inicialización del índice**: Creación con dimensionalidad específica\n",
    "3. **Adición de vectores**: `index.add()` construye estructura de búsqueda interna\n",
    "4. **Serialización**: `faiss.write_index()` persiste índice en disco\n",
    "\n",
    "#### Archivos de Persistencia Generados\n",
    "\n",
    "1. **playbook_index.faiss**: Índice FAISS binario\n",
    "   - Contenido: Estructura de búsqueda vectorial optimizada\n",
    "   - Tamaño: ~6MB para 865 vectores de 1536 dimensiones\n",
    "\n",
    "2. **embeddings_openai.pkl**: DataFrame completo serializado\n",
    "   - Contenido: Datos originales con embeddings\n",
    "   - Formato: Pickle binario de Python\n",
    "   - Uso: Respaldo completo y análisis posterior\n",
    "\n",
    "3. **mapping.csv**: Mapeo de índices a texto\n",
    "   - Contenido: Índice numérico → texto del fragmento\n",
    "   - Formato: CSV con columnas idx, sentencias\n",
    "   - Uso: Conversión de resultados de búsqueda a texto legible\n",
    "\n",
    "### Métricas de Construcción\n",
    "\n",
    "- **Número de vectores indexados**: Cantidad total de embeddings en el índice\n",
    "- **Tamaño del índice**: Espacio en disco utilizado por el archivo FAISS\n",
    "- **Tiempo de construcción**: Duración del proceso de indexación\n",
    "- **Integridad**: Verificación de correspondencia entre índice y datos fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10fca8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos generados exitosamente:\n",
      "- sentencias.csv: 587 filas\n",
      "- mapping.csv: 587 filas\n",
      "- embeddings_openai.pkl: 587 embeddings\n",
      "- playbook_index.faiss: índice FAISS con 587 vectores\n"
     ]
    }
   ],
   "source": [
    "# Convertir embeddings a array de numpy\n",
    "embs = np.vstack(sentences[\"embedding\"].values)\n",
    "\n",
    "# Crear índice FAISS\n",
    "index = faiss.IndexFlatL2(embs.shape[1])\n",
    "index.add(embs)\n",
    "faiss.write_index(index, \"playbook_index.faiss\")\n",
    "\n",
    "# Guardar el mapeo\n",
    "sentences.reset_index(drop=True, inplace=True)\n",
    "with open(\"embeddings_openai.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)\n",
    "\n",
    "sentences[[\"sentencias\"]].to_csv(\"mapping.csv\", index_label=\"idx\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Archivos generados exitosamente:\")\n",
    "print(f\"- sentencias.csv: {len(sentences)} filas\")\n",
    "print(f\"- mapping.csv: {len(sentences)} filas\")\n",
    "print(f\"- embeddings_openai.pkl: {len(sentences)} embeddings\")\n",
    "print(f\"- playbook_index.faiss: índice FAISS con {len(sentences)} vectores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d8dfd5b-20d2-4ca8-b851-92de9f10599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [0.01975308, -0.0040036766, -0.011540718, -0.0...\n",
      "1    [0.025226578, 0.034367915, 0.05278244, 0.01799...\n",
      "2    [0.012700311, 0.037788995, 0.08627299, 0.03251...\n",
      "3    [0.0037709642, 0.073237345, 0.079091996, -0.01...\n",
      "4    [0.06110719, 0.03821712, 0.07568045, 0.0316591...\n",
      "Name: embedding, dtype: object\n",
      "embedding\n",
      "<class 'numpy.ndarray'>    587\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sentences[\"embedding\"].head())\n",
    "print(sentences[\"embedding\"].apply(type).value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
