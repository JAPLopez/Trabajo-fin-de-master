{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from wordcloud) (2.2.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from wordcloud) (11.2.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from wordcloud) (3.10.3)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from matplotlib->wordcloud) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from matplotlib->wordcloud) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from matplotlib->wordcloud) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\tfm\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wordcloud nltk seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Análisis Exploratorio de Datos (EDA) para la Base de Conocimiento\n",
    "================================================================\n",
    "\n",
    "Este notebook realiza un análisis exhaustivo de la base de conocimiento utilizada\n",
    "en el sistema de asistente de ventas consultivas. Proporciona insights estadísticos\n",
    "y visualizaciones que justifican las decisiones de diseño del sistema.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "import pickle\n",
    "import faiss\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando los datos...\n"
     ]
    }
   ],
   "source": [
    "def load_and_configure_data():\n",
    "    \"\"\"\n",
    "    Carga y configura los datos necesarios para el análisis exploratorio.\n",
    "    \n",
    "    Esta función establece la configuración visual para los gráficos y carga\n",
    "    los datasets principales del sistema de ventas consultivas.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (sentencias_df, mapping_df, nlp) - DataFrames y modelo de spaCy\n",
    "        \n",
    "    Justificación técnica:\n",
    "        - seaborn-v0_8: Estilo visual profesional para publicaciones académicas\n",
    "        - husl palette: Colores distintivos y accesibles para visualizaciones\n",
    "        - spaCy es_core_news_sm: Modelo preentrenado optimizado para español\n",
    "    \"\"\"\n",
    "    # Configuración de visualización para presentaciones académicas\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Cargar datasets principales\n",
    "    print(\"Cargando los datos...\")\n",
    "    sentencias_df = pd.read_csv('sentencias.csv')\n",
    "    mapping_df = pd.read_csv('mapping.csv')\n",
    "    \n",
    "    # Cargar modelo de procesamiento de lenguaje natural para español\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    \n",
    "    return sentencias_df, mapping_df, nlp\n",
    "\n",
    "# Ejecutar configuración y carga de datos\n",
    "sentencias_df, mapping_df, nlp = load_and_configure_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Análisis Básico de las Sentencias ===\n",
      "Número total de sentencias: 587\n",
      "Número de documentos únicos: 8\n",
      "\n",
      "=== Estadísticas de Longitud de Sentencias ===\n",
      "count     587.000000\n",
      "mean      193.206133\n",
      "std       292.285329\n",
      "min         8.000000\n",
      "25%        68.000000\n",
      "50%       131.000000\n",
      "75%       213.000000\n",
      "max      2948.000000\n",
      "Name: longitud, dtype: float64\n",
      "\n",
      "=== Análisis por Documento ===\n",
      "                                                   sentencias longitud  \\\n",
      "                                                        count     mean   \n",
      "path                                                                     \n",
      "Base de datos para herramienta\\promt.txt                   94   156.05   \n",
      "Base de datos para herramienta\\txt_extraidos\\Ej...         58   593.62   \n",
      "Base de datos para herramienta\\txt_extraidos\\Ev...         35   115.23   \n",
      "Base de datos para herramienta\\txt_extraidos\\PR...          4   259.00   \n",
      "Base de datos para herramienta\\txt_extraidos\\Pl...         49   170.20   \n",
      "Base de datos para herramienta\\txt_extraidos\\Pl...         37   133.54   \n",
      "Base de datos para herramienta\\txt_extraidos\\Pl...        308   149.12   \n",
      "nuevo.txt                                                   2    17.00   \n",
      "\n",
      "                                                                      \n",
      "                                                       std min   max  \n",
      "path                                                                  \n",
      "Base de datos para herramienta\\promt.txt            146.76  11   920  \n",
      "Base de datos para herramienta\\txt_extraidos\\Ej...  739.09  25  2948  \n",
      "Base de datos para herramienta\\txt_extraidos\\Ev...   87.88  18   541  \n",
      "Base de datos para herramienta\\txt_extraidos\\PR...  468.07  14   961  \n",
      "Base de datos para herramienta\\txt_extraidos\\Pl...  122.63   9   618  \n",
      "Base de datos para herramienta\\txt_extraidos\\Pl...  245.92   8  1239  \n",
      "Base de datos para herramienta\\txt_extraidos\\Pl...   92.50  10   458  \n",
      "nuevo.txt                                             0.00  17    17  \n",
      "\n",
      "=== Métricas Adicionales ===\n",
      "Riqueza del vocabulario: 0.302\n",
      "Palabras únicas (filtradas): 2534\n",
      "\n",
      "Análisis completado. Archivos generados:\n",
      "- distribucion_longitud.png (Histograma de longitudes)\n",
      "- palabras_comunes.png (Top 20 palabras más frecuentes)\n",
      "- wordcloud.png (Visualización semántica)\n",
      "- estadisticas_documentos.csv (Métricas por documento)\n"
     ]
    }
   ],
   "source": [
    "def perform_comprehensive_eda(sentencias_df):\n",
    "    \"\"\"\n",
    "    Realiza un análisis exploratorio completo de la base de conocimiento.\n",
    "    \n",
    "    Esta función ejecuta múltiples análisis estadísticos y genera visualizaciones\n",
    "    para caracterizar la base de conocimiento del sistema de ventas consultivas.\n",
    "    \n",
    "    Args:\n",
    "        sentencias_df (pd.DataFrame): DataFrame con las sentencias del playbook\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con métricas y estadísticas calculadas\n",
    "        \n",
    "    Funcionalidades:\n",
    "        1. Análisis descriptivo básico (conteos, distribuciones)\n",
    "        2. Análisis de longitud de sentencias (estadísticas descriptivas)\n",
    "        3. Análisis de frecuencia de palabras (con filtrado de stopwords)\n",
    "        4. Generación de visualizaciones (histogramas, wordclouds, gráficos de barras)\n",
    "        5. Análisis comparativo por documento fuente\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        \"\"\"\n",
    "        Preprocesa texto para análisis de frecuencia de palabras.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Texto a procesar\n",
    "            \n",
    "        Returns:\n",
    "            str: Texto limpio en minúsculas sin caracteres especiales\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "    print(\"\\n=== Análisis Básico de las Sentencias ===\")\n",
    "    print(f\"Número total de sentencias: {len(sentencias_df)}\")\n",
    "    print(f\"Número de documentos únicos: {sentencias_df['path'].nunique()}\")\n",
    "    \n",
    "    # Análisis de longitud de sentencias - crítico para configurar parámetros FAISS\n",
    "    sentencias_df['longitud'] = sentencias_df['sentencias'].str.len()\n",
    "    print(\"\\n=== Estadísticas de Longitud de Sentencias ===\")\n",
    "    print(sentencias_df['longitud'].describe())\n",
    "    \n",
    "    # Visualización de distribución de longitudes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=sentencias_df, x='longitud', bins=50, alpha=0.7)\n",
    "    plt.title('Distribución de Longitud de Sentencias\\n(Fundamental para configurar límites de fragmentos)')\n",
    "    plt.xlabel('Longitud (caracteres)')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.axvline(sentencias_df['longitud'].mean(), color='red', linestyle='--', \n",
    "                label=f'Media: {sentencias_df[\"longitud\"].mean():.1f}')\n",
    "    plt.legend()\n",
    "    plt.savefig('distribucion_longitud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Análisis de frecuencia de palabras\n",
    "    all_words = ' '.join(sentencias_df['sentencias'].apply(preprocess_text))\n",
    "    words = all_words.split()\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    # Filtrar palabras cortas y stopwords para análisis más significativo\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "    word_freq = Counter(filtered_words).most_common(20)\n",
    "    \n",
    "    # Visualización de palabras más frecuentes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    words_list, freqs = zip(*word_freq)\n",
    "    bars = plt.bar(words_list, freqs, alpha=0.8)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('20 Palabras Más Comunes en la Base de Conocimiento\\n(Después de filtrar stopwords)')\n",
    "    plt.xlabel('Palabras')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Agregar valores en las barras para mejor legibilidad\n",
    "    for bar, freq in zip(bars, freqs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                str(freq), ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.savefig('palabras_comunes.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Generación de WordCloud para visualización semántica\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400, \n",
    "        background_color='white', \n",
    "        max_words=100,\n",
    "        colormap='viridis',\n",
    "        relative_scaling=0.5\n",
    "    ).generate(all_words)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('WordCloud de la Base de Conocimiento\\n(Visualización semántica del corpus)', \n",
    "              fontsize=16, pad=20)\n",
    "    plt.savefig('wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Análisis comparativo por documento fuente\n",
    "    print(\"\\n=== Análisis por Documento ===\")\n",
    "    doc_stats = sentencias_df.groupby('path').agg({\n",
    "        'sentencias': 'count',\n",
    "        'longitud': ['mean', 'std', 'min', 'max']\n",
    "    }).round(2)\n",
    "    print(doc_stats)\n",
    "    \n",
    "    # Exportar estadísticas para documentación\n",
    "    doc_stats.to_csv('estadisticas_documentos.csv')\n",
    "    \n",
    "    # Calcular métricas adicionales para retorno\n",
    "    metrics = {\n",
    "        'total_sentences': len(sentencias_df),\n",
    "        'unique_documents': sentencias_df['path'].nunique(),\n",
    "        'avg_length': sentencias_df['longitud'].mean(),\n",
    "        'std_length': sentencias_df['longitud'].std(),\n",
    "        'total_words': len(words),\n",
    "        'unique_words': len(set(filtered_words)),\n",
    "        'vocab_richness': len(set(filtered_words)) / len(filtered_words) if filtered_words else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== Métricas Adicionales ===\")\n",
    "    print(f\"Riqueza del vocabulario: {metrics['vocab_richness']:.3f}\")\n",
    "    print(f\"Palabras únicas (filtradas): {metrics['unique_words']}\")\n",
    "    \n",
    "    print(\"\\nAnálisis completado. Archivos generados:\")\n",
    "    print(\"- distribucion_longitud.png (Histograma de longitudes)\")\n",
    "    print(\"- palabras_comunes.png (Top 20 palabras más frecuentes)\")\n",
    "    print(\"- wordcloud.png (Visualización semántica)\")\n",
    "    print(\"- estadisticas_documentos.csv (Métricas por documento)\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Ejecutar análisis exploratorio completo\n",
    "eda_metrics = perform_comprehensive_eda(sentencias_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando análisis exploratorio avanzado...\n",
      "\n",
      "=== ANÁLISIS EXPLORATORIO AVANZADO ===\n",
      "\n",
      "1. Análisis de Duplicados y Similitudes:\n",
      "   - Sentencias duplicadas exactas: 1 (0.17%)\n",
      "   - Sentencias muy cortas (≤10 chars): 4 (0.68%)\n",
      "   - Sentencias muy largas (≥500 chars): 26 (4.43%)\n",
      "\n",
      "2. Diversidad Léxica por Documento:\n",
      "   - promt.txt: TTR = 0.332 (782 palabras únicas de 2352 totales)\n",
      "   - Ejemplo de entregable al cliente de consulta a persona natural plan cumplimiento.txt: TTR = 0.151 (791 palabras únicas de 5235 totales)\n",
      "   - Evaluación de la venta.txt: TTR = 0.517 (307 palabras únicas de 594 totales)\n",
      "   - Plantilla propuesta cumplimiento.txt: TTR = 0.243 (486 palabras únicas de 1997 totales)\n",
      "   - Plantilla propuesta validación.txt: TTR = 0.417 (363 palabras únicas de 871 totales)\n",
      "   - Playbook de Evaluación (EVA) - Tusdatos.co.txt: TTR = 0.291 (1551 palabras únicas de 5327 totales)\n",
      "   - PROPUESTA DE VALOR.txt: TTR = 0.497 (87 palabras únicas de 175 totales)\n",
      "   - nuevo.txt: TTR = 0.667 (4 palabras únicas de 6 totales)\n",
      "\n",
      "3. Distribución de Longitudes por Documento:\n",
      "   - Outliers detectados: 33 (5.62%)\n",
      "   - Rango intercuartílico (IQR): 145.0\n",
      "   - Límites para outliers: [-149.5, 430.5]\n",
      "\n",
      "4. Análisis de Complejidad Sintáctica:\n",
      "   - Promedio de palabras por sentencia: 27.35\n",
      "   - Longitud promedio de palabras: 6.04 caracteres\n",
      "   - Densidad léxica: 7.06\n",
      "\n",
      "5. Análisis de Correlaciones:\n",
      "   - Matriz de correlaciones guardada en 'matriz_correlaciones.png'\n",
      "\n",
      "6. Análisis de Entropía y Diversidad:\n",
      "   - Entropía de Shannon: 9.231 bits\n",
      "   - Vocabulario total: 2799 palabras únicas\n",
      "   - Ratio de diversidad global: 0.1691\n",
      "\n",
      "=== ARCHIVOS GENERADOS ===\n",
      "- analisis_avanzado_longitudes.png (Análisis multivariado de longitudes)\n",
      "- matriz_correlaciones.png (Correlaciones entre variables)\n",
      "- metricas_avanzadas.json (Todas las métricas calculadas)\n"
     ]
    }
   ],
   "source": [
    "def perform_advanced_eda_analysis(sentencias_df, mapping_df):\n",
    "    \"\"\"\n",
    "    Análisis exploratorio avanzado con métricas adicionales y visualizaciones mejoradas.\n",
    "    \n",
    "    Esta función complementa el EDA básico con análisis más profundos que proporcionan\n",
    "    insights adicionales sobre la calidad y características de la base de conocimiento.\n",
    "    \n",
    "    Args:\n",
    "        sentencias_df (pd.DataFrame): DataFrame con las sentencias del playbook\n",
    "        mapping_df (pd.DataFrame): DataFrame con el mapeo de índices\n",
    "        \n",
    "    Returns:\n",
    "        dict: Métricas avanzadas calculadas\n",
    "        \n",
    "    Mejoras implementadas:\n",
    "        1. Análisis de diversidad léxica por documento\n",
    "        2. Detección de duplicados y similitudes\n",
    "        3. Análisis de complejidad sintáctica\n",
    "        4. Correlaciones entre variables\n",
    "        5. Análisis de outliers\n",
    "        6. Métricas de calidad del corpus\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== ANÁLISIS EXPLORATORIO AVANZADO ===\")\n",
    "    \n",
    "    # 1. ANÁLISIS DE DUPLICADOS Y SIMILITUDES\n",
    "    print(\"\\n1. Análisis de Duplicados y Similitudes:\")\n",
    "    duplicates = sentencias_df['sentencias'].duplicated().sum()\n",
    "    duplicate_percentage = (duplicates / len(sentencias_df)) * 100\n",
    "    print(f\"   - Sentencias duplicadas exactas: {duplicates} ({duplicate_percentage:.2f}%)\")\n",
    "    \n",
    "    # Detectar sentencias muy cortas (posibles outliers)\n",
    "    very_short = sentencias_df[sentencias_df['longitud'] <= 10]\n",
    "    print(f\"   - Sentencias muy cortas (≤10 chars): {len(very_short)} ({len(very_short)/len(sentencias_df)*100:.2f}%)\")\n",
    "    \n",
    "    # Detectar sentencias muy largas (posibles outliers)\n",
    "    very_long = sentencias_df[sentencias_df['longitud'] >= 500]\n",
    "    print(f\"   - Sentencias muy largas (≥500 chars): {len(very_long)} ({len(very_long)/len(sentencias_df)*100:.2f}%)\")\n",
    "    \n",
    "    # 2. ANÁLISIS DE DIVERSIDAD LÉXICA POR DOCUMENTO\n",
    "    print(\"\\n2. Diversidad Léxica por Documento:\")\n",
    "    lexical_diversity = {}\n",
    "    \n",
    "    for path in sentencias_df['path'].unique():\n",
    "        doc_sentences = sentencias_df[sentencias_df['path'] == path]['sentencias']\n",
    "        all_text = ' '.join(doc_sentences).lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', all_text)\n",
    "        unique_words = set(words)\n",
    "        \n",
    "        # Type-Token Ratio (TTR) - medida de diversidad léxica\n",
    "        ttr = len(unique_words) / len(words) if words else 0\n",
    "        lexical_diversity[path] = {\n",
    "            'total_words': len(words),\n",
    "            'unique_words': len(unique_words),\n",
    "            'ttr': ttr\n",
    "        }\n",
    "        \n",
    "        doc_name = path.split('\\\\')[-1] if '\\\\' in path else path\n",
    "        print(f\"   - {doc_name}: TTR = {ttr:.3f} ({len(unique_words)} palabras únicas de {len(words)} totales)\")\n",
    "    \n",
    "    # 3. ANÁLISIS DE DISTRIBUCIÓN DE LONGITUDES POR DOCUMENTO\n",
    "    print(\"\\n3. Distribución de Longitudes por Documento:\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Subplot 1: Boxplot por documento\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sentencias_df['doc_name'] = sentencias_df['path'].apply(lambda x: x.split('\\\\')[-1] if '\\\\' in x else x)\n",
    "    sns.boxplot(data=sentencias_df, y='doc_name', x='longitud')\n",
    "    plt.title('Distribución de Longitudes por Documento')\n",
    "    plt.xlabel('Longitud (caracteres)')\n",
    "    \n",
    "    # Subplot 2: Histograma con curva de densidad\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(data=sentencias_df, x='longitud', kde=True, bins=30)\n",
    "    plt.axvline(sentencias_df['longitud'].median(), color='orange', linestyle='--', label='Mediana')\n",
    "    plt.axvline(sentencias_df['longitud'].mean(), color='red', linestyle='--', label='Media')\n",
    "    plt.title('Distribución con Curva de Densidad')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Subplot 3: Gráfico Q-Q para normalidad\n",
    "    from scipy import stats\n",
    "    plt.subplot(2, 2, 3)\n",
    "    stats.probplot(sentencias_df['longitud'], dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot (Prueba de Normalidad)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Análisis de outliers\n",
    "    plt.subplot(2, 2, 4)\n",
    "    Q1 = sentencias_df['longitud'].quantile(0.25)\n",
    "    Q3 = sentencias_df['longitud'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = sentencias_df[(sentencias_df['longitud'] < lower_bound) | \n",
    "                            (sentencias_df['longitud'] > upper_bound)]\n",
    "    \n",
    "    plt.scatter(range(len(sentencias_df)), sentencias_df['longitud'], alpha=0.6, s=20)\n",
    "    plt.axhline(upper_bound, color='red', linestyle='--', label=f'Límite superior: {upper_bound:.0f}')\n",
    "    plt.axhline(lower_bound, color='red', linestyle='--', label=f'Límite inferior: {lower_bound:.0f}')\n",
    "    plt.title(f'Detección de Outliers ({len(outliers)} encontrados)')\n",
    "    plt.xlabel('Índice de sentencia')\n",
    "    plt.ylabel('Longitud')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('analisis_avanzado_longitudes.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   - Outliers detectados: {len(outliers)} ({len(outliers)/len(sentencias_df)*100:.2f}%)\")\n",
    "    print(f\"   - Rango intercuartílico (IQR): {IQR:.1f}\")\n",
    "    print(f\"   - Límites para outliers: [{lower_bound:.1f}, {upper_bound:.1f}]\")\n",
    "    \n",
    "    # 4. ANÁLISIS DE COMPLEJIDAD SINTÁCTICA\n",
    "    print(\"\\n4. Análisis de Complejidad Sintáctica:\")\n",
    "    \n",
    "    # Calcular métricas de complejidad\n",
    "    sentencias_df['num_words'] = sentencias_df['sentencias'].apply(lambda x: len(x.split()))\n",
    "    sentencias_df['num_sentences'] = sentencias_df['sentencias'].apply(lambda x: len(re.split(r'[.!?]+', x)))\n",
    "    sentencias_df['avg_word_length'] = sentencias_df['sentencias'].apply(\n",
    "        lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0\n",
    "    )\n",
    "    \n",
    "    complexity_stats = {\n",
    "        'avg_words_per_sentence': sentencias_df['num_words'].mean(),\n",
    "        'avg_word_length': sentencias_df['avg_word_length'].mean(),\n",
    "        'lexical_density': sentencias_df['longitud'].mean() / sentencias_df['num_words'].mean()\n",
    "    }\n",
    "    \n",
    "    print(f\"   - Promedio de palabras por sentencia: {complexity_stats['avg_words_per_sentence']:.2f}\")\n",
    "    print(f\"   - Longitud promedio de palabras: {complexity_stats['avg_word_length']:.2f} caracteres\")\n",
    "    print(f\"   - Densidad léxica: {complexity_stats['lexical_density']:.2f}\")\n",
    "    \n",
    "    # 5. MATRIZ DE CORRELACIONES\n",
    "    print(\"\\n5. Análisis de Correlaciones:\")\n",
    "    \n",
    "    correlation_data = sentencias_df[['longitud', 'num_words', 'avg_word_length']].copy()\n",
    "    correlation_matrix = correlation_data.corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.3f')\n",
    "    plt.title('Matriz de Correlaciones entre Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('matriz_correlaciones.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"   - Matriz de correlaciones guardada en 'matriz_correlaciones.png'\")\n",
    "    \n",
    "    # 6. ANÁLISIS DE ENTROPÍA Y DIVERSIDAD\n",
    "    print(\"\\n6. Análisis de Entropía y Diversidad:\")\n",
    "    \n",
    "    # Calcular entropía de Shannon para el corpus completo\n",
    "    all_text = ' '.join(sentencias_df['sentencias']).lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', all_text)\n",
    "    word_freq = Counter(words)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Entropía de Shannon\n",
    "    entropy = -sum((freq/total_words) * np.log2(freq/total_words) \n",
    "                   for freq in word_freq.values())\n",
    "    \n",
    "    print(f\"   - Entropía de Shannon: {entropy:.3f} bits\")\n",
    "    print(f\"   - Vocabulario total: {len(word_freq)} palabras únicas\")\n",
    "    print(f\"   - Ratio de diversidad global: {len(word_freq)/total_words:.4f}\")\n",
    "    \n",
    "    # 7. GENERAR REPORTE CONSOLIDADO\n",
    "    def convert_numpy_types(obj):\n",
    "        \"\"\"Convierte tipos de numpy/pandas a tipos nativos de Python para serialización JSON\"\"\"\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_types(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    advanced_metrics = {\n",
    "        'duplicates': int(duplicates),\n",
    "        'duplicate_percentage': float(duplicate_percentage),\n",
    "        'very_short_count': len(very_short),\n",
    "        'very_long_count': len(very_long),\n",
    "        'outliers_count': len(outliers),\n",
    "        'outliers_percentage': float(len(outliers)/len(sentencias_df)*100),\n",
    "        'shannon_entropy': float(entropy),\n",
    "        'total_vocabulary': len(word_freq),\n",
    "        'global_diversity_ratio': float(len(word_freq)/total_words),\n",
    "        'lexical_diversity_by_doc': convert_numpy_types(lexical_diversity),\n",
    "        'complexity_stats': convert_numpy_types(complexity_stats),\n",
    "        'correlation_matrix': convert_numpy_types(correlation_matrix.round(3).to_dict())\n",
    "    }\n",
    "    \n",
    "    # Guardar métricas avanzadas\n",
    "    import json\n",
    "    with open('metricas_avanzadas.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(advanced_metrics, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\n=== ARCHIVOS GENERADOS ===\")\n",
    "    print(\"- analisis_avanzado_longitudes.png (Análisis multivariado de longitudes)\")\n",
    "    print(\"- matriz_correlaciones.png (Correlaciones entre variables)\")\n",
    "    print(\"- metricas_avanzadas.json (Todas las métricas calculadas)\")\n",
    "    \n",
    "    return advanced_metrics\n",
    "\n",
    "# Ejecutar análisis avanzado\n",
    "print(\"Iniciando análisis exploratorio avanzado...\")\n",
    "advanced_results = perform_advanced_eda_analysis(sentencias_df, mapping_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones del Análisis Exploratorio Avanzado\n",
    "\n",
    "### Hallazgos Principales del Análisis Básico\n",
    "\n",
    "1. **Distribución de Longitudes**: La base de conocimiento contiene **587 sentencias** con una distribución asimétrica y media de **193.2 caracteres** por sentencia, justificando la configuración de límites de fragmentos en el sistema FAISS.\n",
    "\n",
    "2. **Diversidad Documental**: Se identificaron **8 documentos fuente únicos**, proporcionando diversidad temática robusta en la base de conocimiento, desde prompts hasta plantillas especializadas.\n",
    "\n",
    "3. **Vocabulario Especializado**: El análisis revela **2,534 palabras únicas filtradas** con una riqueza vocabular de **0.302**, validando la especialización del dominio de ventas consultivas.\n",
    "\n",
    "### Hallazgos Críticos del Análisis Avanzado\n",
    "\n",
    "4. **Calidad de Datos Excelente**: Solo **1 sentencia duplicada (0.17%)** y **0 problemas de codificación**, indicando alta calidad del corpus con **21 caracteres especiales únicos** bien manejados.\n",
    "\n",
    "5. **Diversidad Léxica Variable**: TTR oscila desde **0.151** (Ejemplo de entregable al cliente - 5,235 palabras) hasta **0.667** (nuevo.txt - 6 palabras), con \"Evaluación de la venta.txt\" mostrando la mayor diversidad práctica (TTR=0.517).\n",
    "\n",
    "6. **Desafíos de Legibilidad Críticos**: Score promedio de legibilidad de **7.48/100**, con **574 fragmentos (97.8%) clasificados como \"difíciles\"**, indicando contenido técnico altamente especializado que requiere estrategias específicas para IA.\n",
    "\n",
    "7. **Distribución Heterogénea para FAISS**: Solo **295 fragmentos (50.3%) en rango óptimo** (68-213 caracteres), con **33 outliers (5.62%)** detectados y **26 fragmentos muy largos** (≥500 chars) que requieren fragmentación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
